{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMADEUS CHALLENGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The following challenge is conducted as part of the Kschool Master in Data Science. \n",
    "\n",
    "The challenge was created by Amadeus to asses the Data Science skills of candidates, and it consist in resolving three + two exercises given two files about flight bookings and searches. The exercises are:\n",
    "\n",
    "- **First exercise:** count the number of lines of each file in Python\n",
    "- **Second exercise:** identify the top 10 arrival airports in 2013\n",
    "- **Third exercise:** plot the monthly number of searches or flights arriving at Madrid, Barcelona and Malaga\n",
    "- **Bonus 1:** match searches with bookings and export results in a CSV file\n",
    "- **Bonus 2:** write a web service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8041M\r\n",
      "drwxrwxr-x 4 dsc dsc    1M jun  1 01:22 amadeus_challenge\r\n",
      "-rw-rw-r-- 1 dsc dsc 4049M abr  2  2018 bookings.csv\r\n",
      "-rw-rw-r-- 1 dsc dsc  405M may 29 14:40 bookings_no_duplicates.csv\r\n",
      "drwxrwxr-x 4 dsc dsc    1M may 31 18:10 Repo_challenge\r\n",
      "-rw-rw-r-- 1 dsc dsc 3525M abr  2  2018 searches.csv\r\n",
      "-rw-rw-r-- 1 dsc dsc   63M may 29 14:39 searches_no_duplicates.csv\r\n"
     ]
    }
   ],
   "source": [
    "#Get name of files and size\n",
    "!ls -l --block-size=M ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000011\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../bookings.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20390198\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../searches.csv | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the files is too big to load it at once in memory, hence the files will need to be loaded as iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries used\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "#Plot inside jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "#Show all columns printing pandas objects\n",
    "pd.options.display.max_columns = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Count the number of lines in Python for each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files as iterator (due to memory constraints) with only one column to reduce the size of the file\n",
    "bookings_reader=pd.read_csv(\"../bookings.csv\", delimiter=\"^\", iterator=True, chunksize = 10000, usecols=[0])\n",
    "searches_reader=pd.read_csv(\"../searches.csv\", delimiter=\"^\", iterator=True, chunksize = 10000, usecols=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the length dataframe given its iterator\n",
    "def file_lines (file):\n",
    "    \"\"\"Returns the length of a given file that has been read with the itereator\"\"\"\n",
    "    length = 0\n",
    "    for b_chunk in file:\n",
    "        length += b_chunk.shape[0] #Adding the rows of each segment\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_len = file_lines(bookings_reader)\n",
    "searches_len = file_lines(searches_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines of 'bookings.csv': 10,000,010\n",
      "Total lines of 'searches.csv': 20,390,198\n"
     ]
    }
   ],
   "source": [
    "print(\"Total lines of 'bookings.csv': {:,}\".format(bookings_len))\n",
    "print(\"Total lines of 'searches.csv': {:,}\".format(searches_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of lines is:\n",
    "\n",
    "- bookings.csv:  10000010 lines + header\n",
    "- searches.csv: 20390198 lines + header\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying and Cleaning Duplicates (if found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify if there are duplicates, if we could load all the data into one DF we could call the methodn. duplicated() and check if there is any True value, however since we have to load the data into df chunks, it is possible that there are no duplicates within a chunk, but there are whithin different chunks.\n",
    "\n",
    "For this reason, and since finding one duplicate means that there duplicates, we will follow this strategy to identify duplicates:\n",
    "\n",
    "- First, we will check for duplicates using bash\n",
    "- Then, we will check on Python. In python we will first check if we find duplicates within the first 20 chunks. If we find duplicates within the 20 first chunks, we will proceed remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking duplicates in bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359005\r\n"
     ]
    }
   ],
   "source": [
    "#Unique lines (searches.csv)\n",
    "!sort -u -t \"^\" ../searches.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20390199\r\n"
     ]
    }
   ],
   "source": [
    "#total lines (searches.csv)\n",
    "!sort -t \"^\" ../searches.csv| wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000004\r\n"
     ]
    }
   ],
   "source": [
    "#Unique lines (bookings.csv)\n",
    "!sort -u -t \"^\" ../bookings.csv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000011\r\n"
     ]
    }
   ],
   "source": [
    "#total lines (bookings.csv)\n",
    "!sort -t \"^\" ../bookings.csv| wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading files with iterator\n",
    "searches_reader=pd.read_csv(\"../searches.csv\", \n",
    "                            delimiter=\"^\", \n",
    "                            iterator=True, \n",
    "                            chunksize = 10000)\n",
    "bookings_reader=pd.read_csv(\"../bookings.csv\", \n",
    "                            delimiter=\"^\", \n",
    "                            iterator=True, \n",
    "                            chunksize = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searches duplicates: True\n",
      "Bookings duplicates: False\n"
     ]
    }
   ],
   "source": [
    "#Reading the first chunk of each file\n",
    "s_duplicate = False\n",
    "b_duplicate = False\n",
    "for i in range(0,20):\n",
    "    s_chunk = searches_reader.get_chunk()\n",
    "    b_chunk = bookings_reader.get_chunk()\n",
    "\n",
    "    if True in s_chunk.duplicated().values:\n",
    "        s_duplicate = True\n",
    "    if True in b_chunk.duplicated().values:\n",
    "        b_duplicate = True\n",
    "        \n",
    "print(\"Searches duplicates: {}\".format(s_duplicate))\n",
    "print(\"Bookings duplicates: {}\".format(b_duplicate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for duplicates within the chunks have resulted in identifying that searches.csv contains duplicates, and bookings.csv does not. However, as we explained before scoring True in this test proves that there are duplicates, but scoring False does not prove the opposite, is for this reason that both files will be treated for duplicates.\n",
    "\n",
    "The method that we will be following is creating a hash table for each line in the files, and then iterating over the file, and creating a new file with only one occurrence of each hash as indicated by Stackoverlow user \"zwer\" in the following thread: https://stackoverflow.com/questions/52407474/removing-duplicates-on-very-large-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates (input_file,output_file):\n",
    "    with open(input_file, \"r\") as f_in, \\\n",
    "            open(output_file, \"w\") as f_out:\n",
    "        seen = set()  # a set to hold our 'visited' lines\n",
    "        for line in f_in:  # iterate over the input file line by line\n",
    "            line_hash = hashlib.md5(line.encode()).digest()  # hash the value\n",
    "            if line_hash not in seen:  # we're seeing this line for the first time\n",
    "                seen.add(line_hash)  # add it to the hash table\n",
    "                f_out.write(line)  # write the line to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_duplicates(\"../searches.csv\",\"../searches_no_duplicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_duplicates(\"../bookings.csv\",\"../bookings_no_duplicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches_lines_n_dup=file_lines(pd.read_csv(\"../searches_no_duplicates.csv\", \n",
    "                            delimiter=\"^\", \n",
    "                            iterator=True, \n",
    "                            chunksize = 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_lines_n_dup=file_lines(pd.read_csv(\"../bookings_no_duplicates.csv\", \n",
    "                            delimiter=\"^\", \n",
    "                            iterator=True, \n",
    "                            chunksize = 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOKINGS.CSV \n",
      "------------\n",
      "Duplicated lines:  9,000,007\n",
      "Total lines     :  1,000,003\n",
      "\n",
      "\n",
      "SEARCHES.CSV \n",
      "------------\n",
      "Duplicated lines: 20,031,194\n",
      "Total lines     :    359,004\n"
     ]
    }
   ],
   "source": [
    "print(\"BOOKINGS.CSV \\n------------\")\n",
    "print(\"Duplicated lines: {:10,}\".format(bookings_len - bookings_lines_n_dup))\n",
    "print(\"Total lines     : {:10,}\".format(bookings_lines_n_dup))\n",
    "\n",
    "\n",
    "print(\"\\n\\nSEARCHES.CSV \\n------------\")\n",
    "print(\"Duplicated lines: {:10,}\".format(searches_len - searches_lines_n_dup))\n",
    "print(\"Total lines     : {:10,}\".format(searches_lines_n_dup))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been found file bookings.csv cointained 9M duplicates, and searches.csv 20M duplicates.\n",
    "\n",
    "Therefore, the total number of lines is:\n",
    "- bookings.csv: 1,000,003 + header\n",
    "- searches.csv:   359,004 + header\n",
    "\n",
    "\n",
    "*M- Million*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Top 10 arrivals  [no duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read file including only \"arr_port\" and \"pax\" to calculate top 10 airports\n",
    "booking_reader=pd.read_csv(\"../bookings_no_duplicates.csv\", usecols=[\"arr_port\",\"pax\"],delimiter=\"^\", iterator=True, chunksize = 10000 )#names=[arr_port,pax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all the chuncks and group by arrival airport aggregating by bookings passangers (\"pax\")\n",
    "pieces_bookings = [x.groupby(\"arr_port\").sum() for x in booking_reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate all the pieces\n",
    "agg_bookings = pd.concat(pieces_bookings).groupby(level=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arr_port\n",
       "LHR    8881.0\n",
       "MCO    7093.0\n",
       "LAX    7053.0\n",
       "LAS    6963.0\n",
       "JFK    6627.0\n",
       "CDG    6449.0\n",
       "BKK    5946.0\n",
       "MIA    5815.0\n",
       "SFO    5800.0\n",
       "DXB    5559.0\n",
       "Name: pax, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reset index, strip the names, and assign back the index\n",
    "agg_bookings.reset_index(inplace=True)\n",
    "agg_bookings[\"arr_port\"]=agg_bookings[\"arr_port\"].str.strip()\n",
    "agg_bookings.set_index(\"arr_port\", inplace=True)\n",
    "#print top 10 airports per number of passangers\n",
    "agg_bookings.iloc[:,0].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing airport names from GeoBases selecting only columns iata_code, airport name and country name\n",
    "url=\"https://raw.githubusercontent.com/opentraveldata/geobases/public/GeoBases/DataSources/Airports/GeoNames/airports_geonames_only_clean.csv\"\n",
    "columns=[\"iata_code\",\"airport_name\",\"country_name\"]\n",
    "iata_names = pd.read_csv(url, delimiter =\"^\",usecols=[0,1,4],header=None, names=columns)\n",
    "\n",
    "#strip iata_codes and set as index\n",
    "iata_names[\"iata_code\"]=iata_names[\"iata_code\"].str.strip()\n",
    "iata_names.set_index(\"iata_code\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pax</th>\n",
       "      <th>airport_name</th>\n",
       "      <th>country_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arr_port</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LHR</th>\n",
       "      <td>8881.0</td>\n",
       "      <td>London Heathrow Airport</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCO</th>\n",
       "      <td>7093.0</td>\n",
       "      <td>Orlando International Airport</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAX</th>\n",
       "      <td>7053.0</td>\n",
       "      <td>Los Angeles International Airport</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAS</th>\n",
       "      <td>6963.0</td>\n",
       "      <td>McCarran International Airport</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JFK</th>\n",
       "      <td>6627.0</td>\n",
       "      <td>John F Kennedy International Airport</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDG</th>\n",
       "      <td>6449.0</td>\n",
       "      <td>Paris - Charles-de-Gaulle</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BKK</th>\n",
       "      <td>5946.0</td>\n",
       "      <td>Suvarnabhumi</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIA</th>\n",
       "      <td>5815.0</td>\n",
       "      <td>Miami International Airport</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SFO</th>\n",
       "      <td>5800.0</td>\n",
       "      <td>San Francisco International Airport</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DXB</th>\n",
       "      <td>5559.0</td>\n",
       "      <td>Dubai International Airport</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pax                          airport_name          country_name\n",
       "arr_port                                                                    \n",
       "LHR       8881.0               London Heathrow Airport        United Kingdom\n",
       "MCO       7093.0         Orlando International Airport         United States\n",
       "LAX       7053.0     Los Angeles International Airport         United States\n",
       "LAS       6963.0        McCarran International Airport         United States\n",
       "JFK       6627.0  John F Kennedy International Airport         United States\n",
       "CDG       6449.0             Paris - Charles-de-Gaulle                France\n",
       "BKK       5946.0                          Suvarnabhumi              Thailand\n",
       "MIA       5815.0           Miami International Airport         United States\n",
       "SFO       5800.0   San Francisco International Airport         United States\n",
       "DXB       5559.0           Dubai International Airport  United Arab Emirates"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge files on idex to include names of country and airprt\n",
    "agg_bookings_iata_names=agg_bookings.join(iata_names, how=\"left\")\n",
    "\n",
    "#print top 10 airports per number of passangers including airport name and country\n",
    "agg_bookings_iata_names.sort_values(by=\"pax\",ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIRD EXERCISE: plot the monthly number of searches for flights arriving at Malaga, Madrid or Barcelona. [no duplicates]\n",
    "\n",
    "Plot one curve for Malaga, another for Madrid and another for Barcelona in the same fig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore searches file to select only relevant columns\n",
    "searches_reader=pd.read_csv(\"../searches_no_duplicates.csv\", delimiter=\"^\", iterator=True, chunksize = 10000)\n",
    "searches_df=searches_reader.get_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>TxnCode</th>\n",
       "      <th>OfficeID</th>\n",
       "      <th>Country</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>RoundTrip</th>\n",
       "      <th>NbSegments</th>\n",
       "      <th>Seg1Departure</th>\n",
       "      <th>Seg1Arrival</th>\n",
       "      <th>Seg1Date</th>\n",
       "      <th>Seg1Carrier</th>\n",
       "      <th>Seg1BookingCode</th>\n",
       "      <th>Seg2Departure</th>\n",
       "      <th>Seg2Arrival</th>\n",
       "      <th>Seg2Date</th>\n",
       "      <th>Seg2Carrier</th>\n",
       "      <th>Seg2BookingCode</th>\n",
       "      <th>Seg3Departure</th>\n",
       "      <th>Seg3Arrival</th>\n",
       "      <th>Seg3Date</th>\n",
       "      <th>Seg3Carrier</th>\n",
       "      <th>Seg3BookingCode</th>\n",
       "      <th>Seg4Departure</th>\n",
       "      <th>Seg4Arrival</th>\n",
       "      <th>Seg4Date</th>\n",
       "      <th>Seg4Carrier</th>\n",
       "      <th>Seg4BookingCode</th>\n",
       "      <th>Seg5Departure</th>\n",
       "      <th>Seg5Arrival</th>\n",
       "      <th>Seg5Date</th>\n",
       "      <th>Seg5Carrier</th>\n",
       "      <th>Seg5BookingCode</th>\n",
       "      <th>Seg6Departure</th>\n",
       "      <th>Seg6Arrival</th>\n",
       "      <th>Seg6Date</th>\n",
       "      <th>Seg6Carrier</th>\n",
       "      <th>Seg6BookingCode</th>\n",
       "      <th>From</th>\n",
       "      <th>IsPublishedForNeg</th>\n",
       "      <th>IsFromInternet</th>\n",
       "      <th>IsFromVista</th>\n",
       "      <th>TerminalID</th>\n",
       "      <th>InternetOffice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>20:25:57</td>\n",
       "      <td>MPT</td>\n",
       "      <td>624d8c3ac0b3a7ca03e3c167e0f48327</td>\n",
       "      <td>DE</td>\n",
       "      <td>TXL</td>\n",
       "      <td>AUH</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>TXL</td>\n",
       "      <td>AUH</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>D2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AUH</td>\n",
       "      <td>TXL</td>\n",
       "      <td>2013-02-02</td>\n",
       "      <td>D2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ASIWS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>FRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>10:15:33</td>\n",
       "      <td>MPT</td>\n",
       "      <td>b0af35b31588dc4ab06d5cf2986e8e02</td>\n",
       "      <td>MD</td>\n",
       "      <td>ATH</td>\n",
       "      <td>MIL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ATH</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ASIWS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>KIV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>18:04:49</td>\n",
       "      <td>MPT</td>\n",
       "      <td>3561a60621de06ab1badc8ca55699ef3</td>\n",
       "      <td>US</td>\n",
       "      <td>ICT</td>\n",
       "      <td>SFO</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ICT</td>\n",
       "      <td>SFO</td>\n",
       "      <td>2013-08-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SFO</td>\n",
       "      <td>ICT</td>\n",
       "      <td>2013-08-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ASIWS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>17:42:40</td>\n",
       "      <td>FXP</td>\n",
       "      <td>1864e5e8013d9414150e91d26b6a558b</td>\n",
       "      <td>SE</td>\n",
       "      <td>RNB</td>\n",
       "      <td>ARN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RNB</td>\n",
       "      <td>ARN</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>DU</td>\n",
       "      <td>W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ASI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>STO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>17:48:29</td>\n",
       "      <td>MPT</td>\n",
       "      <td>1ec336348f44207d2e0027dc3a68c118</td>\n",
       "      <td>NO</td>\n",
       "      <td>OSL</td>\n",
       "      <td>MAD</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>OSL</td>\n",
       "      <td>MAD</td>\n",
       "      <td>2013-03-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAD</td>\n",
       "      <td>OSL</td>\n",
       "      <td>2013-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ASIWS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d41d8cd98f00b204e9800998ecf8427e</td>\n",
       "      <td>OSL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time TxnCode                          OfficeID Country  \\\n",
       "0  2013-01-01  20:25:57     MPT  624d8c3ac0b3a7ca03e3c167e0f48327      DE   \n",
       "1  2013-01-01  10:15:33     MPT  b0af35b31588dc4ab06d5cf2986e8e02      MD   \n",
       "2  2013-01-01  18:04:49     MPT  3561a60621de06ab1badc8ca55699ef3      US   \n",
       "3  2013-01-01  17:42:40     FXP  1864e5e8013d9414150e91d26b6a558b      SE   \n",
       "4  2013-01-01  17:48:29     MPT  1ec336348f44207d2e0027dc3a68c118      NO   \n",
       "\n",
       "  Origin Destination  RoundTrip  NbSegments Seg1Departure Seg1Arrival  \\\n",
       "0    TXL         AUH          1           2           TXL         AUH   \n",
       "1    ATH         MIL          0           1           ATH         MIL   \n",
       "2    ICT         SFO          1           2           ICT         SFO   \n",
       "3    RNB         ARN          0           1           RNB         ARN   \n",
       "4    OSL         MAD          1           2           OSL         MAD   \n",
       "\n",
       "     Seg1Date Seg1Carrier Seg1BookingCode Seg2Departure Seg2Arrival  \\\n",
       "0  2013-01-26          D2             NaN           AUH         TXL   \n",
       "1  2013-01-04         NaN             NaN           NaN         NaN   \n",
       "2  2013-08-02         NaN             NaN           SFO         ICT   \n",
       "3  2013-01-02          DU               W           NaN         NaN   \n",
       "4  2013-03-22         NaN             NaN           MAD         OSL   \n",
       "\n",
       "     Seg2Date Seg2Carrier Seg2BookingCode Seg3Departure Seg3Arrival Seg3Date  \\\n",
       "0  2013-02-02          D2             NaN           NaN         NaN      NaN   \n",
       "1         NaN         NaN             NaN           NaN         NaN      NaN   \n",
       "2  2013-08-09         NaN             NaN           NaN         NaN      NaN   \n",
       "3         NaN         NaN             NaN           NaN         NaN      NaN   \n",
       "4  2013-03-31         NaN             NaN           NaN         NaN      NaN   \n",
       "\n",
       "  Seg3Carrier Seg3BookingCode Seg4Departure Seg4Arrival Seg4Date Seg4Carrier  \\\n",
       "0         NaN             NaN           NaN         NaN      NaN         NaN   \n",
       "1         NaN             NaN           NaN         NaN      NaN         NaN   \n",
       "2         NaN             NaN           NaN         NaN      NaN         NaN   \n",
       "3         NaN             NaN           NaN         NaN      NaN         NaN   \n",
       "4         NaN             NaN           NaN         NaN      NaN         NaN   \n",
       "\n",
       "  Seg4BookingCode Seg5Departure Seg5Arrival Seg5Date Seg5Carrier  \\\n",
       "0             NaN           NaN         NaN      NaN         NaN   \n",
       "1             NaN           NaN         NaN      NaN         NaN   \n",
       "2             NaN           NaN         NaN      NaN         NaN   \n",
       "3             NaN           NaN         NaN      NaN         NaN   \n",
       "4             NaN           NaN         NaN      NaN         NaN   \n",
       "\n",
       "  Seg5BookingCode Seg6Departure Seg6Arrival Seg6Date Seg6Carrier  \\\n",
       "0             NaN           NaN         NaN      NaN         NaN   \n",
       "1             NaN           NaN         NaN      NaN         NaN   \n",
       "2             NaN           NaN         NaN      NaN         NaN   \n",
       "3             NaN           NaN         NaN      NaN         NaN   \n",
       "4             NaN           NaN         NaN      NaN         NaN   \n",
       "\n",
       "  Seg6BookingCode    From  IsPublishedForNeg  IsFromInternet  IsFromVista  \\\n",
       "0             NaN  1ASIWS                  0               0            0   \n",
       "1             NaN  1ASIWS                  0               0            0   \n",
       "2             NaN  1ASIWS                  0               0            0   \n",
       "3             NaN    1ASI                  0               0            0   \n",
       "4             NaN  1ASIWS                  0               0            0   \n",
       "\n",
       "                         TerminalID InternetOffice  \n",
       "0  d41d8cd98f00b204e9800998ecf8427e            FRA  \n",
       "1  d41d8cd98f00b204e9800998ecf8427e            KIV  \n",
       "2  d41d8cd98f00b204e9800998ecf8427e            NYC  \n",
       "3  d41d8cd98f00b204e9800998ecf8427e            STO  \n",
       "4  d41d8cd98f00b204e9800998ecf8427e            OSL  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unconverted data remains: ,10:15:33,MPT,b0af35b31588dc4ab06d5cf2986e8e02,MD,ATH,MIL,0,1,ATH,MIL,2013-01-04,,,,,,,,,,,,,,,,,,,,,,,,,,,,1ASIWS,0,0,0,d41d8cd98f00b204e9800998ecf8427e,KIV",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mconverter\u001b[0;34m(*date_cols)\u001b[0m\n\u001b[1;32m   3020\u001b[0m                 result = tools.to_datetime(\n\u001b[0;32m-> 3021\u001b[0;31m                     date_parser(*date_cols), errors='ignore')\n\u001b[0m\u001b[1;32m   3022\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-143eca7174cd>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Function to parse dates and times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdateparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: strptime() argument 1 must be str, not numpy.ndarray",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mconverter\u001b[0;34m(*date_cols)\u001b[0m\n\u001b[1;32m   3029\u001b[0m                                                 \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_parser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m                                                 dayfirst=dayfirst),\n\u001b[0m\u001b[1;32m   3031\u001b[0m                         errors='ignore')\n",
      "\u001b[0;32mpandas/_libs/tslibs/parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.try_parse_dates\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.try_parse_dates\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-143eca7174cd>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Function to parse dates and times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdateparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    576\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    361\u001b[0m         raise ValueError(\"unconverted data remains: %s\" %\n\u001b[0;32m--> 362\u001b[0;31m                           data_string[found.end():])\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unconverted data remains: ,10:15:33,MPT,b0af35b31588dc4ab06d5cf2986e8e02,MD,ATH,MIL,0,1,ATH,MIL,2013-01-04,,,,,,,,,,,,,,,,,,,,,,,,,,,,1ASIWS,0,0,0,d41d8cd98f00b204e9800998ecf8427e,KIV",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-143eca7174cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                             \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelevant_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                             \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                             date_parser=dateparse)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1919\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1921\u001b[0;31m             \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_date_conversions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1922\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_do_date_conversions\u001b[0;34m(self, names, data)\u001b[0m\n\u001b[1;32m   1673\u001b[0m             data, names = _process_date_conversion(\n\u001b[1;32m   1674\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_date_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m                 self.index_names, names, keep_date_col=self.keep_date_col)\n\u001b[0m\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_process_date_conversion\u001b[0;34m(data_dict, converter, parse_spec, index_col, index_names, columns, keep_date_col)\u001b[0m\n\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m             _, col, old_names = _try_convert_dates(converter, colspec,\n\u001b[0;32m-> 3085\u001b[0;31m                                                    data_dict, orig_names)\n\u001b[0m\u001b[1;32m   3086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3087\u001b[0m             \u001b[0mnew_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_try_convert_dates\u001b[0;34m(parser, colspec, data_dict, columns)\u001b[0m\n\u001b[1;32m   3115\u001b[0m     \u001b[0mto_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolnames\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3117\u001b[0;31m     \u001b[0mnew_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mto_parse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mconverter\u001b[0;34m(*date_cols)\u001b[0m\n\u001b[1;32m   3031\u001b[0m                         errors='ignore')\n\u001b[1;32m   3032\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3033\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mgeneric_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdate_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3035\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/date_converters.py\u001b[0m in \u001b[0;36mgeneric_parser\u001b[0;34m(parse_func, *cols)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-143eca7174cd>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Function to parse dates and times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdateparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Import only relevant columns, merging dates and parsing it to datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    576\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         raise ValueError(\"unconverted data remains: %s\" %\n\u001b[0;32m--> 362\u001b[0;31m                           data_string[found.end():])\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0miso_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unconverted data remains: ,10:15:33,MPT,b0af35b31588dc4ab06d5cf2986e8e02,MD,ATH,MIL,0,1,ATH,MIL,2013-01-04,,,,,,,,,,,,,,,,,,,,,,,,,,,,1ASIWS,0,0,0,d41d8cd98f00b204e9800998ecf8427e,KIV"
     ]
    }
   ],
   "source": [
    "#Select Relevant columns\n",
    "relevant_columns=[\"Date\",\"Destination\"]\n",
    "\n",
    "#Function to parse dates and times \n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d')\n",
    "\n",
    "#Import only relevant columns, merging dates and parsing it to datetime\n",
    "searches_df=pd.read_csv(\"../searches_no_duplicates.csv\", \n",
    "                            delimiter=\"^\", \n",
    "                            usecols=relevant_columns,\n",
    "                            parse_dates={'datetime': ['Date']},\n",
    "                            date_parser=dateparse)\n",
    "\n",
    "\n",
    "\n",
    "#Explore the type of the columns\n",
    "searches_df.info()\n",
    "searches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering Madrid (MAD), Barcelona(BCN) & Malaga(AGP)\n",
    "pieces = []\n",
    "\n",
    "#Clean destination file\n",
    "searches_df[\"Destination\"] = searches_df[\"Destination\"].str.strip()\n",
    "query= (searches_df[\"Destination\"]==\"MAD\") | (searches_df[\"Destination\"]==\"BCN\") | (searches_df[\"Destination\"]==\"AGP\")\n",
    "searches_esp= searches_df[query]\n",
    "\n",
    "#Convert dates to datetime\n",
    "searches_esp[\"Date\"] = pd.to_datetime(searches_esp[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = searches_esp[searches_esp[\"Destination\"]==\"MAD\"].resample(\"M\", on=\"Date\").count()\n",
    "to_plot[\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = {\"MAD\":\"Madrid\",\"BCN\":\"Barcelona\",\"AGP\":\"Malaga\"}\n",
    "months = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Ago\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "\n",
    "#Create figure\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig.suptitle('Number of flight searches per arriving city, per month in 2013', fontsize=20)\n",
    "\n",
    "#Aggregate and plot\n",
    "for i,air in enumerate(airports):\n",
    "    ax = fig.add_subplot(2,2,i+1)\n",
    "    to_plot = searches_esp[searches_esp[\"Destination\"]==air].resample(\"M\", on=\"Date\").count()\n",
    "    to_plot[\"Date\"].plot(kind=\"bar\",ax=ax, title=airports[air], rot=45, color=\"grey\", legend=None)\n",
    "    ax.set_xticklabels(months)\n",
    "    ax.xaxis.set_label_text(\"\")\n",
    "    ax.set_ylim(0,600)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new = pd.DataFrame()\n",
    "for i,air in enumerate(airports):\n",
    "    to_plot = searches_esp[searches_esp[\"Destination\"]==air].resample(\"M\", on=\"Date\").count()[\"Destination\"]\n",
    "    new[airports[air]]=to_plot\n",
    "new.index=months\n",
    "new.plot(kind=\"bar\",figsize=(10,5), rot=45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS EXERCISE: match searches with bookings [no duplicates]\n",
    "\n",
    "For every search in the searches file, find out whether ended up in a booking or not.\n",
    "Generate a csv with the searches with an additional column containing 1 if the search ended up in a booking or 0 in any other case.\n",
    "\n",
    "origin and destination in the search should match with the booking\n",
    "\n",
    "Process:\n",
    "1. Identify relevant columns to from both files.\n",
    "2. Add a column with \"1\"s to bookings df\n",
    "3. Left join bookings into searches by the columns identifiedin step 1\n",
    "4. Assign 0 to \"NaN\"s on the new column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify columns to compare in both files.\n",
    "\n",
    "First, since we dont have a column description file, lets explore the data again and try to identify common columns in both files. Intuition suggest that we will need to find matching flight details, and date, since the same flight can occure multiple times a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the files with iterator\n",
    "booking_reader=pd.read_csv(\"../bookings_no_duplicates.csv\", \n",
    "                            delimiter=\"^\",\n",
    "                            iterator = True,\n",
    "                            chunksize = 1000)\n",
    "\n",
    "searches_reader=pd.read_csv(\"../searches_no_duplicates.csv\", \n",
    "                            delimiter=\"^\",\n",
    "                            iterator = True,\n",
    "                            chunksize = 1000)\n",
    "\n",
    "#Reading the first block of the files\n",
    "booking_chunk = booking_reader.get_chunk()\n",
    "searches_chunk = searches_reader.get_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the columns from whitespaces before and after for bookings\n",
    "booking_chunk.columns=booking_chunk.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that may be interesting from bookings.csv file:\n",
    "\n",
    "Flight relate columns:\n",
    "- **dep_port**: departure airport\n",
    "- **arr_port**: arrival airport\n",
    "- **mkt_port**: ?\n",
    "- **route**: Starting airport to arrival airport, including transfer airport - one way\n",
    "- **carrier**: airline\n",
    "- **lst_port**: returning airpot\n",
    "- **brd_port**: ?\n",
    "- **off_port**: ?\n",
    "\n",
    "\n",
    "Date related columns:\n",
    "- **act_date**: unknown\n",
    "- **cre_date**: creation date - when the booking is performed ?\n",
    "- **brd_time**: boarding time?\n",
    "- **off_time**: arrival time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches_chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that may be interesting from searches.csv file:\n",
    "\n",
    "Flight relate columns:\n",
    "- **Origin**: departure airport\n",
    "- **Destination**: arrival airport\n",
    "- **SegXDeparture**: departure airport per segment\n",
    "- **SegXArrival**: arrival airport per segment\n",
    "- **SegXCarrier**: airline of the segment\n",
    "\n",
    "Date related columns:\n",
    "- **Date**: search date\n",
    "- **Time**: search time\n",
    "- **SegXDate**: time of the flight per segment\n",
    "\n",
    "It seems that file searches.csv contains additional detail to bookings.csv; it seems to include all the intermediary transfers, including return details. \n",
    "\n",
    "- Does this mean that a return-flight search could create two booking lines? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching possibilities:\n",
    "- For FLIGHT data\n",
    "    - \"Origin\" (searches.csv) seems to have a direct match with \"dep_port\" (bookings.csv)\n",
    "    - \"Destination\" (searches.csv) seems to have a direct match with \"arr_port\" (bookings.csv)\n",
    "    - \"route\" seems to be the combination of all \"segXDeparture\" (bookings.csv) excluding return segments. \n",
    "    - \"carrier\" seems to be related with \"SegXCarrier\" however, when there are multiple segments in the searches.csv file, there could be multiple carriers and it is not clear which airline could corresponde to the carrier in the bookings.csv file. \n",
    "    \n",
    "- For DATE data (more confusing)\n",
    "    - \"Date\" (searches.csv) could be related to \"cre_date\", but its not clear \n",
    "    - \"SegXDate\" could be related to \"brd_time\", but its not clear.\n",
    "    \n",
    "**Matching possibilities for the flight, are clear. However, without a matching date, any route search would create a booking possitive to all the bookings with the same route / Origin & Destination, depending on the matching strategy.**\n",
    "\n",
    "Lets do additional research to confirm what dates match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading complete files without duplicates\n",
    "booking_df=pd.read_csv(\"../bookings_no_duplicates.csv\", \n",
    "                            delimiter=\"^\")\n",
    "\n",
    "searches_df=pd.read_csv(\"../searches_no_duplicates.csv\", \n",
    "                            delimiter=\"^\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the columns from whitespaces before and after for bookings\n",
    "booking_df.columns=booking_chunk.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_df[\"route\"].value_counts()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets select RHOLED, which should be easy to find in searches.csv, if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_df[booking_df[\"route\"].str.strip()==\"RHOLED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches_df[(searches_df[\"Origin\"]==\"RHO\")&(searches_df[\"Destination\"]==\"LED\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were semi-lucky!, we found a matched record, however... none of the dates match... This could mean two things:\n",
    "- The search does not correspond to the booking.\n",
    "- We dont have enough information to match both files by DATE.\n",
    "\n",
    "And without a \"readme\" file with information about the columns, we would need to continue researching until we find a match, or we conclude that there is not a direct match between date.\n",
    "\n",
    "**Since this is a theoretical exercise, lets merege the files by:**\n",
    "- **\"Origin\"(searches.csv) == \"dep_port\" (bookings.csv)**\n",
    "- **\"Destination\" (searches.csv) == \"arr_port\" (bookings.csv)**\n",
    "- **\"Date\" (searches.csv) == \"cre_date\" (bookings.csv)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for merging - UNCOMPLETE\n",
    "\n",
    "pending: clean date columns, and merge using date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading\n",
    "booking_df=pd.read_csv(\"../bookings_no_duplicates.csv\", \n",
    "                            delimiter=\"^\",\n",
    "                            usecols=[\"arr_port\",\"dep_port\",\"cre_date           \"])\n",
    "\n",
    "searches_df=pd.read_csv(\"../searches_no_duplicates.csv\", \n",
    "                            delimiter=\"^\",\n",
    "                            usecols=[\"Origin\",\"Destination\",\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_df.columns=booking_df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting date column\n",
    "def to_date (x):\n",
    "    if len(x)==10:\n",
    "        return x\n",
    "    elif len(x)>10:\n",
    "        return x[0:10]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "booking_df[\"cre_date\"]=booking_df[\"cre_date\"].map(to_date)\n",
    "searches_df[\"Date\"]=searches_df[\"Date\"].map(to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Nan\n",
    "print(\"NaN in bookings: {}\".format(booking_df.isnull().sum().sum()))\n",
    "print(\"NaN in searches: {}\".format(searches_df.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bookings has no NaN, however searches contains 2 NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select rows with NAN\n",
    "print(searches_df[searches_df.isnull().any(axis=1)])\n",
    "print(booking_df[booking_df.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since is only one, and doesnt contain nor Oriign nor Destiantion, we can safely drop it \n",
    "searches_df.dropna(inplace=True)\n",
    "booking_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip columns\n",
    "booking_df[\"arr_port\"]=booking_df[\"arr_port\"].str.split().astype(str)\n",
    "booking_df[\"dep_port\"]=booking_df[\"dep_port\"].str.split().astype(str)\n",
    "searches_df[\"Origin\"]=searches_df[\"Origin\"].str.split().astype(str)\n",
    "searches_df[\"Destination\"]=searches_df[\"Destination\"].str.split().astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicates in bookings to keep shape whem merging with searches\n",
    "booking_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booking_df[\"booked\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging booking to searches\n",
    "merged = pd.merge(searches_df,booking_df, how=\"left\",left_on=[\"Origin\",\"Destination\",\"Date\"], right_on=[\"dep_port\",\"arr_port\",\"cre_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"booked\"].fillna(0,inplace=True)\n",
    "merged[\"booked\"]=merged[\"booked\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"booked\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the selected matching strategy, out of the 359,003 unique lines inside searches.csv, 12.778 created a booking.\n",
    "\n",
    "It is important to note that the filteres used were:\n",
    "\n",
    "- \"Origin\"(searches.csv) == \"dep_port\" (bookings.csv)\n",
    "- \"Destination\" (searches.csv) == \"arr_port\" (bookings.csv)\n",
    "- \"Date\" (searches.csv) == \"cre_date\" (bookings.csv)\n",
    "\n",
    "Lets complete the exercise by merging searches column \"booked\" to the complete searches.csv (without duplicates), and saving it to csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read complete file seraches-csv\n",
    "searches_df_all_columns=pd.read_csv(\"../searches_no_duplicates.csv\", \n",
    "                            delimiter=\"^\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge booked column to searches\n",
    "searches_df_all_columns[\"booked\"]=merged[\"booked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print first rows of the merged file\n",
    "searches_df_all_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to file\n",
    "searches_df_all_columns.to_csv(\"searches_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, merged results are saved into searches_2.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
